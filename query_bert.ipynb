{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def tokenize_input(inputStr, tokenizer, seq_length=512):\n",
    "    pad_id = 0\n",
    "    tokenized_sentence = tokenizer.encode(inputStr)[:seq_length-20]\n",
    "    tokens = tokenized_sentence\n",
    "    token_length = len(tokens)\n",
    "    tokens.extend([pad_id] * (seq_length - token_length))\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    return tokens.reshape(1,seq_length), [token_length]\n",
    "\n",
    "\n",
    "TEMP=\"temp/\"\n",
    "bert_model_path = '/home/gzy/Documents/lora/models/roberta-m-s_12L_cn'\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_model_path, cache_dir=TEMP)\n",
    "\n",
    "WRAPPED_MODEL = BertForMaskedLM.from_pretrained(\n",
    "            bert_model_path,\n",
    "            from_tf=False,\n",
    "            config=bert_config,\n",
    "            cache_dir=TEMP,\n",
    "        )\n",
    "for param in WRAPPED_MODEL.parameters():\n",
    "    param.requires_grad = True\n",
    "WRAPPED_MODEL.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "WRAPPED_MODEL.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "WRAPPED_MODEL.to(device)\n",
    "print('device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputStr = '我想睡觉了'\n",
    "input, lengths = tokenize_input(inputStr, tokenizer, seq_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = WRAPPED_MODEL(input.cuda()).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 21128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:31<00:00, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors num 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "data = np.load('./data/eastmoney_full_stocks_list_nlu_tencent.pkl', allow_pickle=True)\n",
    "\n",
    "all = []\n",
    "\n",
    "for idx in tqdm(range(len(data))):\n",
    "    try:\n",
    "        content = data[idx]['baike_content'] + data[idx]['baike_summary']\n",
    "        tokens, token_length = tokenize_input(content, tokenizer, seq_length=512)\n",
    "        output = WRAPPED_MODEL(tokens.to(device)).logits\n",
    "        vector = output[0,token_length[0]].detach().cpu().numpy()\n",
    "        one = {\n",
    "            'ticker_id': data[idx]['ticker_id'],\n",
    "            'ticker_name': data[idx]['ticker_name'],\n",
    "            'vector': vector\n",
    "        }\n",
    "        all.append(one)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('vectors num', len(all))\n",
    "with open('./data_ignore/vectors_bert.pkl','wb') as f:\n",
    "    pickle.dump(all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query by vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 369510.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = np.load('./data_ignore/vectors_bert.pkl', allow_pickle=True)\n",
    "\n",
    "ticker_names = []\n",
    "ticker_ids = []\n",
    "vectors = []\n",
    "\n",
    "for idx in tqdm(range(len(data))):\n",
    "    if 'vector' in data[idx].keys():\n",
    "        vectors.append(data[idx]['vector'])\n",
    "        ticker_ids.append(data[idx]['ticker_id'])\n",
    "        ticker_names.append(data[idx]['ticker_name'])\n",
    "\n",
    "vectors = np.stack(vectors)\n",
    "\n",
    "def similarity_vector_matrix(arr, brr):\n",
    "    return arr.dot(brr.T) / (np.sqrt(np.sum(arr*arr)) * np.sqrt(np.sum(brr*brr, axis=1)))\n",
    "\n",
    "def stock_search(query, topk=10):\n",
    "\n",
    "    tokens, token_length = tokenize_input(query, tokenizer, seq_length=512)\n",
    "    output = WRAPPED_MODEL(tokens.to(device)).logits\n",
    "    vector = output[0,token_length[0]].detach().cpu().numpy()\n",
    "\n",
    "    res = similarity_vector_matrix(vector, vectors)\n",
    "    idxs = np.argsort(res)[::-1]\n",
    "\n",
    "    topk_idxs = idxs[:topk]\n",
    "    names = [ticker_names[idx] for idx in topk_idxs]\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京沪高铁', '锦江酒店', '阿尔特', '安科瑞', '中材科技', '北新建材', '华发股份', '康希诺', '星宇股份', '润阳科技']\n"
     ]
    }
   ],
   "source": [
    "query = '自动驾驶，新能源汽车'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中国石化', '鄂尔多斯', '行动教育', '锦江酒店', '福莱特', '宁波银行', '京沪高铁', '北方华创', '报喜鸟', '曲美家居']\n"
     ]
    }
   ],
   "source": [
    "query = '电影，电视剧，文化艺术'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中国石化', '锦江酒店', '行动教育', '鄂尔多斯', '福莱特', '京沪高铁', '报喜鸟', '一品红', '探路者', '斯迪克']\n"
     ]
    }
   ],
   "source": [
    "query = '啤酒，烧烤，朋友聚会'\n",
    "stock_search(query)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15d51a80c8523c0d895fb35f7540c75abd446de76bbe33959811c01c8d0e84fa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
