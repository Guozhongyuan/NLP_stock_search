{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = np.load('./data/eastmoney_full_stocks_list_nlu_tencent.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\"Score\": 1.0, \"Word\": \"公司\"},\n",
       " {\"Score\": 0.88982016, \"Word\": \"医疗器械\"},\n",
       " {\"Score\": 0.69142354, \"Word\": \"浙江省\"},\n",
       " {\"Score\": 0.6476816, \"Word\": \"针\"},\n",
       " {\"Score\": 0.619902, \"Word\": \"注射针\"},\n",
       " {\"Score\": 0.6144252, \"Word\": \"制造\"},\n",
       " {\"Score\": 0.58027697, \"Word\": \"技术\"},\n",
       " {\"Score\": 0.57011515, \"Word\": \"产品\"},\n",
       " {\"Score\": 0.56897545, \"Word\": \"用户\"},\n",
       " {\"Score\": 0.56668705, \"Word\": \"品种\"},\n",
       " {\"Score\": 0.55956155, \"Word\": \"行业\"},\n",
       " {\"Score\": 0.5518364, \"Word\": \"康德莱集团\"},\n",
       " {\"Score\": 0.5193484, \"Word\": \"单位\"},\n",
       " {\"Score\": 0.502113, \"Word\": \"企业\"},\n",
       " {\"Score\": 0.45472413, \"Word\": \"高分子\"},\n",
       " {\"Score\": 0.45368028, \"Word\": \"分会\"},\n",
       " {\"Score\": 0.45321786, \"Word\": \"理事长\"},\n",
       " {\"Score\": 0.39482775, \"Word\": \"高新技术\"},\n",
       " {\"Score\": 0.3924612, \"Word\": \"厂家\"},\n",
       " {\"Score\": 0.36895323, \"Word\": \"试点\"},\n",
       " {\"Score\": 0.36446825, \"Word\": \"称号\"},\n",
       " {\"Score\": 0.35745487, \"Word\": \"发展\"},\n",
       " {\"Score\": 0.34765783, \"Word\": \"融合\"},\n",
       " {\"Score\": 0.33645958, \"Word\": \"示范\"},\n",
       " {\"Score\": 0.3358695, \"Word\": \"制造业\"},\n",
       " {\"Score\": 0.33228558, \"Word\": \"系列\"},\n",
       " {\"Score\": 0.32576707, \"Word\": \"非洲\"},\n",
       " {\"Score\": 0.3225992, \"Word\": \"南美\"},\n",
       " {\"Score\": 0.32228953, \"Word\": \"中东\"},\n",
       " {\"Score\": 0.3219856, \"Word\": \"欧洲\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['text_keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_all = []\n",
    "\n",
    "for one in data:\n",
    "    for item in one['text_keyword']:\n",
    "        keywords_all.append(item.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14674"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_set = list(set(keywords_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4757"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 819.47it/s]\n"
     ]
    }
   ],
   "source": [
    "all = []\n",
    "\n",
    "for one in tqdm(data):\n",
    "    vector = np.zeros(len(keywords_set))\n",
    "    for item in one['text_keyword']:\n",
    "        word = item.Word\n",
    "        idx = keywords_set.index(word)\n",
    "        vector[idx] = 1\n",
    "    all.append(\n",
    "        {\n",
    "            'ticker_id': one['ticker_id'],\n",
    "            'ticker_name': one['ticker_name'],\n",
    "            'vector': vector,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./data_ignore/vectors_keyword_Bag.pkl','wb') as f:\n",
    "    pickle.dump(all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = np.load('./data_ignore/vectors_keyword_Bag.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 326729.04it/s]\n"
     ]
    }
   ],
   "source": [
    "ticker_names = []\n",
    "ticker_ids = []\n",
    "vectors = []\n",
    "\n",
    "for idx in tqdm(range(len(data))):\n",
    "    if 'vector' in data[idx].keys():\n",
    "        vectors.append(data[idx]['vector'])\n",
    "        ticker_ids.append(data[idx]['ticker_id'])\n",
    "        ticker_names.append(data[idx]['ticker_name'])\n",
    "\n",
    "vectors = np.stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_vector_matrix(arr, brr):\n",
    "    return arr.dot(brr.T) / (np.sqrt(np.sum(arr*arr)) * np.sqrt(np.sum(brr*brr, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "\n",
    "def stock_search(query, topk=10):\n",
    "\n",
    "    tokens = jieba.lcut(query)\n",
    "    vector = np.zeros(len(keywords_set))\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            idx = keywords_set.index(token)\n",
    "            vector[idx] = 1\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    res = similarity_vector_matrix(vector, vectors)\n",
    "    idxs = np.argsort(res)[::-1]\n",
    "\n",
    "    topk_idxs = idxs[:topk]\n",
    "    names = [ticker_names[idx] for idx in topk_idxs]\n",
    "\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自动驾驶，新能源汽车\n",
    "- 电影，电视剧，文化艺术\n",
    "- 啤酒，烧烤，朋友聚会\n",
    "- 医疗保险，重大疾病保障\n",
    "- 新冠肺炎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['阿尔特', '杉杉股份', '宁德时代', '上汽集团', '双环传动', '比亚迪', '三峡能源', '天齐锂业', '华阳集团', '天奇股份']\n"
     ]
    }
   ],
   "source": [
    "query = '自动驾驶，新能源汽车'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['光线传媒', '中国建筑', '水井坊', '孩子王', '中科电气', '新城控股', '东方雨虹', '泸州老窖', '太阳纸业', '索菲亚']\n"
     ]
    }
   ],
   "source": [
    "query = '电影，电视剧，文化艺术'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['重庆啤酒', '燕京啤酒', '来伊份', '珠江啤酒', '青岛啤酒', '华阳国际', '中科电气', '药明康德', '爱尔眼科', '长城汽车']\n"
     ]
    }
   ],
   "source": [
    "query = '啤酒，烧烤，朋友聚会'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['广和通', '科威尔', '中国船舶', '爱尔眼科', '华阳集团', '小熊电器', '中科电气', '药明康德', '长城汽车', '爱施德']\n"
     ]
    }
   ],
   "source": [
    "query = '医疗保险，重大疾病保障'\n",
    "stock_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['康希诺', '中国船舶', '千味央厨', '天赐材料', '华策影视', '赞宇科技', '伟星股份', '美的集团', '华阳国际', '长城汽车']\n"
     ]
    }
   ],
   "source": [
    "query = '新冠肺炎'\n",
    "stock_search(query)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15d51a80c8523c0d895fb35f7540c75abd446de76bbe33959811c01c8d0e84fa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
